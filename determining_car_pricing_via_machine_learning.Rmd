---
title: "Determining Car Pricing Via Machine Learning"
author: "Zack Reardon"
date: "10/10/2022"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Through a regression-approach predictive machine learning model, the aim is to determine the asking price of a particular vehicle with minimum error.

### Data

The dataset I am using includes data from approximately 250,000 used vehicles advertised for sale in the UK between 2012 and 2021. Each vehicle is identified by its make, model, year of advertisement, month of advertisement, color, year of registration, body type, mileage, engine size, transmission type, fuel type, price, number of seats, and number of doors among other things. I have obtained this dataset from [DVM-CAR](https://deepvisualmarketing.github.io) and converted it into an Excel file. Although this file was too large to push, the citation is available in the data folder in GitHub.

### Purpose

I am interested in predicting the asking price of a given vehicle in the United Kingdom. In particular, I hope that my model is able to aid in determining the expected value of a vehicle on behalf of a seller or the expected cost of a vehicle for a buyer within the market. While the data I am using relates to the car market in the UK, a similar model could be used to predict car pricing in the United States or elsewhere, provided a sufficient dataset is available.

## Loading Packages

Before conducting EDA and modelling on our dataset, we will first install and load the required packages for implementation within the R program.

```{r, message = FALSE}
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(corrplot)
library(ggthemes)
library(scales)
library(glmnet)
library(thinkr)
library(randomForest)
library(kknn)
library(kernlab)
library(tune)
tidymodels_prefer()
```

## Loading and Cleaning Dataset

In order to render the data usable in R, I first cleaned the data in Excel to ensure it could be read effectively. The process of cleaning the data included removing any rows in Excel where one or more of the corresponding columns contained empty or unknown parameters. Additionally, I altered the original predictor Engin_size to `Engin_size_L` where instead of the engine capacity followed by "L" to represent liters, the new predictor would just show the engine capacity in the number of liters. This allows for the variable to be treated as a quantitative value for the purpose of comparisons. All in all, the number of vehicle data points decreased from 268,255 to 235,097 which is still a significantly large sample size for our purposes. I was then able to import the data into RStudio.

```{r}
library("readxl")
UK_Car_Ad_Dataset <- read_excel("/Users/zackreardon/Documents/Data/UK_Car_Ad_table.xlsx")
```

Let's quickly visualize the data to get a sense of the variables involved in the initial dataset.
```{r}
UK_Car_Ad_Dataset %>%
  # display portion of initial dataset
  head()
```

Since advertised month and advertised year are only relevant in the context of each other, appending the advertised month onto the advertised year as a decimal should make for a more practical correlation between the time of advertising and the other predictors/response. I will go about this by subtracting 1 from the month, dividing it by 12, and adding that fraction to the associated year. This should combine the advertised year and month into a single relationship which can be used as a new predictor variable. The advertised month and advertised year will be dropped from the dataset in favor of the new variable `Adv_time` representing the time of advertisement.

```{r}
UK_Car_Ad_Dataset <- UK_Car_Ad_Dataset %>%
  # create new variable combining advertisement year and month
  mutate(Adv_time = Adv_year + ((Adv_month-1)/12)) %>%
  # remove initial variables representing advertisement year and advertisement month
  select(-Adv_year, -Adv_month)
```

The variables Genmodel_ID and Adv_ID are not relevant outside of the initial use of the compiled data. Furthermore, since I am planning on creating dummy variables for `Genmodel` and the variable `Adv_time` is numerical, Genmodel_ID and Adv_ID are not needed as predictors. Genmodel_ID and Adv_ID will subsequently be removed from the dataset. The remaining variables are exhibited below.

```{r}
UK_Car_Ad_Dataset <- UK_Car_Ad_Dataset %>%
  # remove variables Genmodel_ID and Adv_ID
  select(-Genmodel_ID, -Adv_ID)

# show variables in dataset
sapply(UK_Car_Ad_Dataset, class)
```
Finally, we will convert all "character" predictors to factors and clean the level names in `Genmodel` for dummy coding later.

```{r}
# converting to factors
UK_Car_Ad_Dataset$Maker <- as.factor(UK_Car_Ad_Dataset$Maker)
UK_Car_Ad_Dataset$Genmodel <- as.factor(UK_Car_Ad_Dataset$Genmodel)
UK_Car_Ad_Dataset$Color <- as.factor(UK_Car_Ad_Dataset$Color)
UK_Car_Ad_Dataset$Bodytype <- as.factor(UK_Car_Ad_Dataset$Bodytype)
UK_Car_Ad_Dataset$Gearbox <- as.factor(UK_Car_Ad_Dataset$Gearbox)
UK_Car_Ad_Dataset$Fuel_type <- as.factor(UK_Car_Ad_Dataset$Fuel_type)

# cleaning levels
UK_Car_Ad_Dataset$Genmodel <- clean_levels(UK_Car_Ad_Dataset$Genmodel)
```

### Codebook

After adjusting the dataset, the finalized codebook for informational purposes is as follows.

* `Maker`: the manufacturer of the advertised vehicle
* `Genmodel`: the model name of the advertised vehicle
* `Color`: the color of the advertised vehicle
* `Reg_year`: the year that the advertised vehicle was registered
* `Bodytype`: the type of vehicle (e.g. SUV, Convertible, Pickup, etc.)
* `Runned_Miles`: the mileage of the advertised vehicle
* `Engin_size_L`: the capacity of the engine in liters
* `Gearbox`: the vehicle's transmission type (Automatic, Semi-Automatic, or Manual)
* `Fuel_type`: the type of fuel that the vehicle accepts (e.g. Petrol, Diesel, Electric, etc.)
* `Price`: the advertised price of the vehicle in GBP
* `Seat_num`: the number of seats
* `Door_num`: the number of doors/hatches
* `Adv_time`: the time of the advertisement in year + fraction representing month

A copy of this codebook is located in the data folder in GitHub.

## Splitting Data

The next step is to split the data for use in fitting a model. I stratified the data on the response `Price`, the variable representing the price of the vehicle in question. This ensures that vehicles at all price points are well represented in both the training and testing sets. The data was split with 80% assigned to the training set and 20% assigned to the testing set.
```{r}
# ensure same split is used
set.seed(100)

# split data and stratify on predictor Bodytype
UK_CAD_split <- UK_Car_Ad_Dataset %>%
  initial_split(prop = 0.8, strata = "Price")

# create training and testing sets
UK_CAD_train <- training(UK_CAD_split)
UK_CAD_test <- testing(UK_CAD_split)
```

```{r}
# verifying number of observations in training and testing sets
dim(UK_CAD_train)
dim(UK_CAD_test)
```
The number of observations in the training set is 188,076 while the number of observations in the testing set is 47,021.

## Exploratory Data Analysis

Before creating a model, it is important to explore the relationships between variables within the training data.

### Manufacturer

Intuitively, I would expect a correlation between manufacturer (`Maker`) and the price (`Price`) of a used vehicle. Let's visualize the average advertised price based on manufacturer.
```{r, message = FALSE}
UK_CAD_train %>%
  # group by manufacturer
  group_by(Maker) %>%
  # create variable representing average price
  summarise(Avg_price = mean(Price, na.rm=TRUE)) %>%
  # plot average price by manufacturer
  ggplot(aes(x=reorder(Maker,-Avg_price), y=Avg_price)) + 
  geom_col(fill='red', width=0.25) +
  theme(axis.text.x = element_text(angle=90)) +
  ggtitle("Average Advertised Price by Manufacturer") +
  xlab("Manufacturer") +
  ylab("Average Price") +
  # adjust notation for y-axis
  scale_y_continuous(labels = comma)
```

As expected, manufacturers such as McLaren, Lamborghini, and Ferrari have the highest average advertised price while smaller Malaysian and Korean brands have the lowest average advertised price. There appears to be a large correlation between brand and price based on this metric.

### Mileage

People tend to avoid higher-mileage vehicles when in the used car search. Let's see the distribution of the variable `Runned_Miles` to highlight how many high-mileage vehicles are on the market.

```{r}
ggplot(UK_CAD_train, aes(Runned_Miles)) +
  # capping max at 250,000 for visualization purposes
  geom_histogram(binwidth=5, breaks = seq(0, 250000, 500)) +
  labs(title="Histogram of Mileage") +
  xlab("Mileage") +
  ylab("Count") +
  # adjust notation for x-axis
  scale_x_continuous(labels = comma)
```

Overall the amount of vehicles being advertised decreases as the mileage increases. In particular, a large portion of the vehicles are advertised with 0 miles. This is probably due to new cars being advertised through dealerships. Interestingly, there appears to be periodic and consistent spikes at various levels of mileage. I would interpret these to indicate intuitive estimates rather than direct reports from the vehicle's odometer (e.g. 105,000 miles vs 103,521 miles).

Since there appears to be fewer cars offered with higher mileage, lets visualize the correlation between mileage and the advertised price.

```{r, message = FALSE}
ggplot(UK_CAD_train, aes(x=Runned_Miles, y=Price)) +
  geom_point(size=0.05, shape=1) +
  labs(title="Mileage vs Price") +
  xlab("Mileage") +
  # adjust notation for x-axis and y-axis
  scale_x_continuous(labels = comma) +
  scale_y_continuous(labels = comma) +
  # cap price at 200,000 and mileage at 250,000 for visualization purposes
  xlim(0,250000) +
  ylim(0,200000)
```

As the mileage of a vehicle increases, the price tends to decrease as expected. The price also exhibits banding as vehicles tend to be advertised at distinct prices such as 15000, 50000, 100000, etc. rather than specific prices such as 15645.

### Year of Registration

The year of registration refers to the year that the vehicle in question was initially registered. Therefore, newer models will typically have a higher value for the year of registration. It seems natural that cars with a newer year of registration will command a higher price than older vehicles.

In order to validate this theory, we will explore the variable `Reg_year`. Let's look at the years of registration that are recorded in the data.

```{r}
registration_years <- unique(UK_CAD_train$Reg_year)
# convert to ascending order
sort(registration_years, decreasing=FALSE)
```

The earliest recorded year is 1990, with cars registered primarily in the 2000s. The most recent year of registration is 2019. Let's compare the year of registration to a vehicle's mileage.
```{r}
# temporarily create new training set with Reg_year as a factor
UK_CAD_train1 <- UK_CAD_train
UK_CAD_train1$Reg_year <- as.factor(UK_CAD_train1$Reg_year)

ggplot(UK_CAD_train1, aes(x=Reg_year, y=Runned_Miles)) +
  geom_boxplot(outlier.color="red") +
  theme(axis.text.x = element_text(angle=90)) +
  labs(title="Registered Year vs Mileage") +
  xlab("Registered Year") +
  ylab("Mileage") +
  # cap mileage at 250,000 for visualization purposes and remove new vehicles
  ylim(1,250000)
```

It appears that an inverse relationship does exist between the year of registration and the mileage of an advertised vehicle. However, there are a lot of outliers in the data meaning that other factors come into play to determine mileage such as personal usage rather than simply time since registration. Since we noted an inverse relationship between `Runned_Miles` and `Price`, and an inverse relationship between `Reg_year` and `Runned_Miles`, we can expect a positive correlation between `Reg_year` and `Price`.

### Color

Before assessing any correlations between `Color` and any other predictors or the response variable, I'd like to visualize the distribution of the `Color` variable. This should provide insight into the popularity of certain colors and highlight market trends.

```{r}
UK_CAD_train %>%
  # plot number of vehicles by color
  ggplot(aes(x=fct_infreq(Color))) + 
  geom_bar() +
  labs(title="Number of Vehicles by Given Color") +
  xlab("Color") +
  ylab("Count") +
  # adjust angle of text on x-axis
  theme(axis.text.x = element_text(angle=90))
```

As would be expected, it appears that basic colors such as silver, black, and white tend to be well represented in the training dataset while specific or more vibrant colors such as burgundy or indigo account for a much smaller portion. When taking resale value into account, the general market advice is that vehicles in these basic colors tend to maintain their value better than those in bolder hues.

To evaluate this theory, let's compare vehicle color to advertised price via the predictor `Color` and the response `Price`. In order to make the comparison fair, we will cap the price at 50,000 to eliminate skew from expensive supercars in exotic colors.

```{r}
# temporarily remove vehicles priced at over 50,000
UK_CAD_train2 = filter(UK_CAD_train, Price<50000)

UK_CAD_train2 %>%
  # group by manufacturer
  group_by(Color) %>%
  # create variable representing average price
  summarise(Avg_price = mean(Price, na.rm=TRUE)) %>%
  # plot average price by manufacturer
  ggplot(aes(x=reorder(Color,-Avg_price), y=Avg_price)) + 
  geom_col(fill="red", width=0.5) +
  theme(axis.text.x = element_text(angle=90)) +
  ggtitle("Average Advertised Price by Vehicle Color") +
  xlab("Color") +
  ylab("Average Price") +
  # adjust notation for y-axis
  scale_y_continuous(labels = comma)
```

Interestingly there appears to be no distinct pattern between color and price, at least within the 50,000 GBP price point. Bright colors as well as more subdued colors are both represented at the higher end of the average price scale. Notably, silver also falls on the lower end of the scale which is unexpected. Within the context of our dataset, the color palette may serve as a less effective predictor of price than I initially thought.

## Fitting Models

### Fold Training Data and Build Recipe

We will cross-validate by folding the training set into 10 folds with 5 repeats and again stratify on `Price`. This allows for the accuracy of the model in question to be assessed on new data within the training set before fitting the model to the testing set. `Runned_Miles` will be prepped to be logged by filtering out values of zero. Fewer than 390 observations will be removed with this step which does not represent a significant decrease in available data within the folds.

```{r}
# filter training data to remove zeros
UK_CAD_train <- UK_CAD_train %>%
  filter(Runned_Miles != 0)

# set seed for repeatability
set.seed(100)

UK_CAD_folds <- vfold_cv(UK_CAD_train, v = 10, repeats = 5, strata = "Price")
```

Before fitting models to the training data, I will construct a recipe that defines the relationship between our predictors and the response `Price`. Interaction variables should be created between `Genmodel` and `Door_num` as well as between `Door_num` and `Seat_num`. I decided to log `Runned_Miles` since its distribution is right-skewed. I will save the recipe in a subfolder of data called modelling available in GitHub. Unfortunately, the training set and the folds are too large to be stored in GitHub.

```{r,eval=FALSE}
# create recipe with Price as the response
UK_CAD_recipe <- recipe(Price ~ Maker + Genmodel + Color + Reg_year + Bodytype + Runned_Miles + Engin_size_L + Gearbox + Fuel_type + Seat_num + Door_num + Adv_time, data = UK_CAD_train) %>%
  # account for new values
  step_novel(all_nominal_predictors()) %>%
  # assign "other" if less numerous than threshold
  step_other(Genmodel, threshold = 1000) %>%
  # dummy coding all categorical predictors
  step_dummy(all_nominal_predictors()) %>%
  # create interaction variables as necessary
  step_interact(~ starts_with("Genmodel"):Door_num) %>%
  step_interact(~ Door_num:Seat_num) %>%
  # log right-skewed numeric variable
  step_log(Runned_Miles, base = 10) %>%
  # filter variables with single value
  step_zv(all_predictors()) %>%
  # center and scale all predictors
  step_normalize(all_predictors())

save(UK_CAD_recipe, file="/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/recipe.rda")
```

Now that we have defined a recipe for our data, we can begin the model fitting process. The models that I will be fitting are lasso regression, random forest, k-nearest neighbors, and support vector machine. While the tuned models are all saved as .rda files within the modelling subfolder locally, these files are too large to push to GitHub.

### Lasso Regression

The first step in fitting a lasso regression to the training data is to set up a lasso regression specification and an associated workflow. Notably, for a lasso regression, we will utilize a linear regression function with `mixture` set equal to 1. We will also tune the penalty to ensure the best available r-squared value.

```{r}
# set specification for lasso regression
lasso_spec <- 
  linear_reg(penalty = tune(), mixture = 1) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

# loading recipe
load("/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/recipe.rda")

# create workflow for lasso regression
lasso_workflow <- workflow() %>% 
  # add recipe created earlier
  add_recipe(UK_CAD_recipe) %>% 
  add_model(lasso_spec)
```

Next we must create a grid for `penalty` that indicates the range of penalty values that we will assess the performance of, as well as the number of levels. For `levels` I have chosen 50, while the range for penalty is set between -3 and 3. I initially implemented a penalty range between -10 and 10 but reduced it for the sake of computing power.

```{r}
# creating grid of penalty for tuning
penalty_grid <- grid_regular(penalty(range = c(-3,3)), levels = 50)
```

We are now able to fit the models by tuning the penalty grid on UK_CAD_folds within our lasso regression workflow.

```{r,eval=FALSE}
# fit resamples with penalty_grid
UK_CAD_lasso_tune <- tune_grid(
  lasso_workflow,
  resamples = UK_CAD_folds, 
  grid = penalty_grid)

save(UK_CAD_lasso_tune, file="/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/lasso_tune.rda")
```
The results are illustrated via the autoplot function.

```{r}
# loading UK_CAD_lasso_tune
load("/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/lasso_tune.rda")

# plotting outcome
autoplot(UK_CAD_lasso_tune)
```

### Random Forest

We will begin the fitting process for a random forest by creating a specification for this model as well as a workflow. I will also prepare hyperparameters for tuning and set the number of trees equal to 1000 which should be sufficient for our data. There is no need to tune the number of trees since a greater number of trees does not induce overfitting in a random forest model.

```{r}
# creating specification for random forest
rf_spec <- rand_forest(mtry = tune(), trees = 1000, min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("regression")

# loading recipe
load("/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/recipe.rda")

# setting up workflow
rf_workflow <- workflow() %>% 
  # add recipe created earlier
  add_recipe(UK_CAD_recipe) %>% 
  add_model(rf_spec)
```
Let's create a tuning grid for the hyperparameters `mtry` and `min_n`. The ranges that I am utilizing for for these parameters are (2,8) and (5,20) respectively. The values for `mtry` must be contained within (1,14) since there are 14 total predictors including the two interaction variables. However, I am not confident that utilizing more than 8 predictors will be beneficial. `levels` is set equal to 2 to manage computing requirements.

```{r}
# create regular tuning grid
hp_grid <- grid_regular(mtry(range=c(2,8)), min_n(range=c(5,20)), levels = 2)
```
Now we can fit our tuning parameters to our resamples to help optimize our random forest model.

```{r,eval=FALSE}
# fitting resamples with tuning grid
UK_CAD_rf_tune <- tune_grid(
  rf_workflow,
  resamples = UK_CAD_folds, 
  grid = hp_grid)

save(UK_CAD_rf_tune, file = "/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/rf_tune.rda")
```
We can visualize the results using the autoplot function.

```{r}
# loading UK_CAD_rf_tune
load("/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/rf_tune.rda")

# plotting outcome
autoplot(UK_CAD_rf_tune)
```

### K-nearest Neighbors

The specification for a k-nearest neighbors model utilizes the nearest_neighbor function from the parsnip package. "K" in k-nearest neighbors refers to the number of neighbors near a data point that will be considered when classifying the group that the data point will belong to. In this case we will tune the `neighbors` parameter to optimize the value “k”. The default values for the other hyperparameters should be sufficient without the need for tuning. I also set up a workflow for the k-nearest neighbors model.

```{r}
# set specification for k-nearest neighbors
knn_spec <- 
  nearest_neighbor(neighbors = tune()) %>%
  set_mode("regression") %>%
  set_engine("kknn")

# loading recipe
load("/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/recipe.rda")

# setting up workflow for k-nearest neighbors
knn_workflow <- workflow() %>% 
  # add recipe created earlier
  add_recipe(UK_CAD_recipe) %>% 
  add_model(knn_spec)
```
In order to tune “k” (neighbors), I created a regular tuning grid. Since the computing power requirement is extremely high for this process given the amount of data, I set the range between the default value of 5 and 10. `levels` is set at 2 to minimize the computing time.

```{r}
# creating tuning grid for neighbors
neighbors_grid <- grid_regular(neighbors(range=c(5,10)), levels = 2)
```
We can now tune neighbors on our folded training data using the k-nearest neighbors workflow.

```{r, eval=FALSE}
# configuring for parallel processing
before_loaded<- names(sessionInfo()$otherPkgs)

# utilizing all available cores
library(doParallel)
cl <- makeCluster(4)
registerDoParallel(cl)

# fitting resamples with tuning grid
UK_CAD_knn_tune <- tune_grid(
  knn_workflow,
  resamples = UK_CAD_folds, 
  grid = neighbors_grid)

# resetting packages after parallel processing
after_loaded<- names(sessionInfo()$otherPkgs)
after_loaded[!(after_loaded %in% before_loaded)]

stopCluster(cl)
registerDoSEQ()

save(UK_CAD_knn_tune, file = "/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/knn_tune.rda")
```
This can be assessed with the autoplot function.

```{r}
# loading UK_CAD_knn_tune
load("/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/knn_tune.rda")

# plotting outcome
autoplot(UK_CAD_knn_tune)
```

### Support Vector Machine

The final model to set the specification of and make a workflow for is SVM. I decided to utilize the `svm_poly` function. This allows for a linear class boundary when assessing our dataset, and `degree` is set equal to 1 accordingly. I will tune the `cost` parameter, representing the cost for the predicted position of a data point relative to the margin. The other hyperparameters will remain as default, and an associated workflow will be constructed.

```{r}
# set specification for SVM
svm_spec <- 
  svm_poly(degree = 1, cost=tune()) %>%
  set_mode("regression") %>%
  set_engine("kernlab")

# loading recipe
load("/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/recipe.rda")

# create workflow for lasso regression
svm_workflow <- workflow() %>% 
  # add recipe created earlier
  add_recipe(UK_CAD_recipe) %>% 
  add_model(svm_spec)
```
Since the parameter `cost` is restricted to positive values, I have set up a regular tuning grid with a range between 0.1 and 1 which should suffice. `levels` has been set at 2.

```{r}
# creating tuning grid for cost
cost_grid <- grid_regular(cost(range=c(0.1,1)), levels = 2)
```
The value for `cost` can now be adequately tuned with the SVM workflow and the folded training data.

```{r,eval=FALSE}
# configuring for parallel processing
before_loaded<- names(sessionInfo()$otherPkgs)

# utilizing all available cores
library(doParallel)
cl <- makeCluster(4)
registerDoParallel(cl)

# fitting resamples with tuning grid
UK_CAD_svm_tune <- tune_grid(
  svm_workflow,
  resamples = UK_CAD_folds, 
  grid = cost_grid)

# resetting packages after parallel processing
after_loaded<- names(sessionInfo()$otherPkgs)
after_loaded[!(after_loaded %in% before_loaded)]

stopCluster(cl)
registerDoSEQ()

save(UK_CAD_svm_tune, file = "/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/svm_tune.rda")
```
Unfortunately, even with parallel processing using my 4 available cores, tuning the SVM model was on track to take several days and could not be achieved by the course deadline. I intend to explore the option of virtual cores including running RStudio on an AWS EC2 instance with a higher core count machine. With a more powerful computer supplemented by virtual cores, the SVM hyperparameter tuning requirements should be reduced to a more reasonable time.

## Finalized Model

### Choosing Best Model

In order to choose the best model to fit on our testing data, we will assess the tuning performance of the lasso regression, random forest, and k-nearest neighbors models. The support vector machine will not be considered for the time being, subject to computing power limitations. The metric that we will be assessing by is rmse, with the best performing parameters from each tuned grid compared to each other. The model with the best rmse of the three will be the best fit for our data.

```{r}
# loading tune rda files
load("/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/lasso_tune.rda")
load("/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/rf_tune.rda")
load("/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/knn_tune.rda")

# visualizing best performing parameter values
show_best(UK_CAD_lasso_tune, metric = "rmse")
show_best(UK_CAD_rf_tune, metric = "rmse")
show_best(UK_CAD_knn_tune, metric = "rmse")
```
It appears that the lowest rmse of 7385 is achieved by k-nearest neighbors with a k value of 5.

### Fitting and Evaluating Model

We will fit the best-performing k-nearest neighbors model to the training and testing sets. The first step is to finalize the k-nearest neighbors workflow with the best parameters. We will then fit this finalized workflow to the whole training dataset. The final model was saved locally but was too large to push to GitHub.

```{r,eval=FALSE}
# finalizing knn workflow
final_workflow <- knn_workflow %>%
  finalize_workflow(select_best(UK_CAD_knn_tune, metric = "rmse"))

# fitting training data
final_model <- fit(final_workflow, UK_CAD_train)

save(final_model, file = "/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/final_model.rda")
```
Finally, let's predict the testing data using the final model we have fit to the training data.

```{r}
load("/Users/zackreardon/Documents/determining_car_pricing_via_machine_learning/data/modelling/final_model.rda")

# filter test set to allow for mileage to be logged
UK_CAD_test <- UK_CAD_test %>% 
  filter(Runned_Miles != 0)

# predicting on the testing set
augment(final_model, new_data = UK_CAD_test) %>%
  rmse(truth = Price, estimate = .pred)

# out of interest let's evaluate the associated r-squared value
augment(final_model, new_data = UK_CAD_test) %>%
  rsq(truth = Price, estimate = .pred)
```
The rmse of our testing data is 46967 which is significantly worse than the rmse from training which was 7385. Out of curiosity I also calculated the r-squared of the final model on the testing data which is 0.167, meaning that the model only accounts for 16.7% of the variability in the data.

## Conclusion

Evidently, this model would provide little real world benefit in terms of vehicle pricing owing to a high rmse and a correspondingly low r-squared value. Given greater computing power and increased time availability, I could have optimized the tuning parameters, as my main objective was to complete the tuning process with the resources I had available. Additionally, I feel that further tweaks to my recipe could help provide more consistent results. However, even with the ideal parameters in place for the perfect model, I'm uncertain that vehicle pricing can be predicted with a large degree of accuracy. My EDA exhibited far more randomization in terms of variables than I had anticipated. For instance, my intuition that black, gray, or white cars are more popular since they command a higher resale value, was not entirely supported by the average price based on color. Since exotic and luxury vehicles exist as outliers compared to daily drivers, modelling these vehicle types separately could have theoretically been more effective.

In terms of model outcomes, k-nearest neighbors proved to be the most accurate, followed by random forest and lasso regression. Unfortunately the support vector machine was unable to be tuned and rendered in time. With the benefit of hindsight, I would have replaced the SVM with a boosted tree model. I feel that with parameter optimization, the performance of the random forest can be increased to exceed the potential of the k-nearest neighbors. Ultimately, all models performed relatively poorly.

I think that this model can be adjusted to be more effective by more finely tuning the parameters for the random forest. I feel that adding more levels for `mtry` between 2 and 8 will be beneficial, while the `min_n` value of 5 seems sufficient. I am confused as to how the k-nearest neighbors model performed better than the random forest, since it should have more difficulty when scaling up to a dataset of our size. It will be interesting to see how the support vector machine does end up performing. With the implementation of either greater computing power, virtual cores, or a combination of the two, a tuned SVM model should be accessible. Computing time for the SVM could have been excessive due to both the large amount of data and the regression rather than classification approach.